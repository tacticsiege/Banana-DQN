{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train agent in environment\n",
    "#0 import and initialize environment, get info\n",
    "#0 import and setup agent(s)\n",
    "# define train_dqn(env, agent, ...)\n",
    "# 1. Train Top Agent \n",
    "#    log: ep, avg. score per 100, min/max/avg, epsilon, duration\n",
    "#    persist: scores, epsilons\n",
    "#    save: /run_name/: checkpoints, final, scores, epsilons\n",
    "#    plot: scores, avg over 100, epsilons, solved_line\n",
    "# 2. Grid Search Training\n",
    "#    input: set of dictionaries 'Param' -> Value or default\n",
    "#    log+: solved?, episode_solved\n",
    "#    persist: top_agent\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from util import env_initialize, env_reset, state_reward_done_unpack\n",
    "from util import EpsilonService\n",
    "from dqn_agent import DQN_Params, DQN_Agent\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# create unity environment\n",
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")\n",
    "\n",
    "# gather scenario information, used globally throughout notebook\n",
    "brain, brain_name, state, action_size, state_size = env_initialize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: uncomment lines below to demo a uniform-random agent\n",
    "#from util import demo_random_agent\n",
    "#demo_random_agent(env, n_episodes=2, train_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: create agent without params\n",
    "params = DQN_Params(name='Default DQN')\n",
    "agent = DQN_Agent(state_size, action_size, seed=seed, params=params)\n",
    "#scores, ep_stats = agent.train(env, n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, agent, run_name='default_dqn', n_episodes=1000, goal_score=15, print_every=100,\n",
    "              eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100) # last 100 episode scores\n",
    "    duration_winow = deque(maxlen=5) # last 5 episode durations\n",
    "    \n",
    "    # initialize epsilon\n",
    "    epsilon_svc = EpsilonService(\n",
    "        method='decay', start_value=eps_start, end_value=eps_end, \n",
    "        decay_rate=eps_decay, n_episodes=n_episodes)\n",
    "    epsilon = epsilon_svc.get_value()\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # reset for new episode        \n",
    "        score = 0\n",
    "        state = env_reset(env, brain_name, train_mode=True)\n",
    "        \n",
    "        # run episode\n",
    "        episode_start_time = time.time()\n",
    "        while True:\n",
    "            # choose and take action\n",
    "            action = int(agent.act(state, epsilon))\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state, reward, done = state_reward_done_unpack(env_info)\n",
    "            \n",
    "            # update agent with new state and reward\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward\n",
    "            \n",
    "            state = next_state # update state for next timestep\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # decay\n",
    "        epsilon = epsilon_svc.update(i_episode)\n",
    "        \n",
    "        # track progress\n",
    "        duration = time.time() - start_time\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        duration_window.append(duration)\n",
    "        \n",
    "        # display progress and save checkpoints\n",
    "        avg_score = np.mean(scores_window)\n",
    "        avg_duration = np.mean(duration_window)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tAvg. Duration: {:.4f}s'.format(\n",
    "            i_episode, avg_score, avg_duration), end=\"\")\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tAvg. Duration: {:.4f}s'.format(i, avg_score, avg_duration))\n",
    "            # todo: save checkpoint weights\n",
    "            \n",
    "        if avg_score > goal_score:\n",
    "            print('\\rEnvironment solved in {} episodes!'.format(i_episode))\n",
    "            print('\\rAverage Score for last 100 episodes: {:.2f}'.format(avg_score))\n",
    "            print('\\rTotal Duration: {:.2f}m'.format(time.time() - training_start_time))\n",
    "            # todo: save solved model weights and print\n",
    "            return\n",
    "    \n",
    "    # finished all episodes\n",
    "    print('\\rComplete training on {} episodes.'.format(n_episodes))\n",
    "    print('\\rAverage Score for last 100 episodes: {:.2f}'.format(np.mean(scores_window)))\n",
    "    print('\\rTotal Duration: {:.2f}m'.format(time.time() - training_start_time))\n",
    "    # todo: save trained model weights and print        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
